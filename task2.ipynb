{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "База данных писем скачана и расположена рядом с файлом ipynb в файле database.sqlite. Посмотрим на раздел Emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Id', 'DocNumber', 'MetadataSubject', 'MetadataTo', 'MetadataFrom', 'SenderPersonId', 'MetadataDateSent', 'MetadataDateReleased', 'MetadataPdfLink', 'MetadataCaseNumber', 'MetadataDocumentClass', 'ExtractedSubject', 'ExtractedTo', 'ExtractedFrom', 'ExtractedCc', 'ExtractedDateSent', 'ExtractedCaseNumber', 'ExtractedDocNumber', 'ExtractedDateReleased', 'ExtractedReleaseInPartOrFull', 'ExtractedBodyText', 'RawText']\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "conn = sqlite3.connect('database.sqlite')\n",
    "c = conn.cursor()\n",
    "c.execute(\"SELECT * FROM Emails\")\n",
    "field_names = [i[0] for i in c.description]\n",
    "print(field_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нас интересует поле RawText, в котором хранятся тексты писем. Извлечем эти данные из базы данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Emails = c.fetchall()\n",
    "Emails2 = [tuple(elm,) for elm in Emails]\n",
    "Emails3 = pd.DataFrame(Emails2, columns = [u'Id', u'DocNumber', u'MetadataSubject', u'MetadataTo', u'MetadataFrom', u'SenderPersonId', u'MetadataDateSent', u'MetadataDateReleased', u'MetadataPdfLink', u'MetadataCaseNumber', u'MetadataDocumentClass', u'ExtractedSubject', u'ExtractedTo', u'ExtractedFrom', u'ExtractedCc', u'ExtractedDateSent', u'ExtractedCaseNumber', u'ExtractedDocNumber', u'ExtractedDateReleased', u'ExtractedReleaseInPartOrFull', u'ExtractedBodyText', u'RawText'])\n",
    "\n",
    "emails = Emails3.RawText.values.tolist()\n",
    "text = ' '.join([word for word in emails])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предобработка текста. Вначале удалим из текста все вспомогательные символы, которые не несут смысловой нагрузки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "import re\n",
    "import numpy as np\n",
    "def rmUselessSymbols(texts): \n",
    "    ctext = re.sub(r'[[]$$~`*=-_+&#(){}<>,\\.!?;:\\'\"/\\\\\\_|%', '', texts) \n",
    "    ctext = ctext.replace(\"^\", \" \")\n",
    "    ctext = ctext.replace(\"°\", \" \")\n",
    "    ctext = ctext.replace(\"•\", \" \")\n",
    "    return ctext\n",
    "EmailsNonAlpha = rmUselessSymbols(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрев на текст, можно увидеть стандартный текст во многим письмах. Вырежем его также. Уменьшим размер датасета до 1000 для приемлимости времени работы программы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "EmailsNonAlpha = EmailsNonAlpha.split(\"UNCLASSIFIED\")\n",
    "EmailsList = []\n",
    "for i in range(1000):\n",
    "    EmailsList.append(EmailsNonAlpha[i])\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def deleteTechText(text):\n",
    "    text = re.sub(r\"<[\\w\\.-]+@[\\w\\.-]+>\", \"EMAIL\", text.lower())\n",
    "    text = re.sub(r\"([0-9]|0[0-9]|1[0-9]|2[0-3])(:|-)[0-5][0-9]\", \"\", text) \n",
    "    text = re.sub(r\" [0-9]+\\.?[0-9]*\", \" \", text)\n",
    "    text = re.sub(r\"https?:\\\\/\\\\/(www\\\\.)?[-a-zA-Z0-9@:%._\\\\+~#=]{2,256}\\\\.[a-z]{2,6}\\\\b([-a-zA-Z0-9@:%_\\\\+.~#?&//=]*)\", \"\", text)\n",
    "    \n",
    "    text = re.sub(\"\\d\", \" \", text)\n",
    "    text = re.sub(r\"\\W+\", \" \", text).strip()\n",
    "    text = (lemmatizer.lemmatize(word) for word in text.split() if len(word) > 3)\n",
    "    text = [word for word in text if not (word in stop_words)] # remove stop words\n",
    "    tags = nltk.pos_tag(text)\n",
    "    text = \" \".join([tag[0] for tag in tags if (\"NN\" in tag[1]) or (\"VB\" in tag[1])])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordsBigr = \" \".join(deleteTechText(fragment) for fragment in EmailsList).strip().split()\n",
    "msg_temp = (deleteTechText(fragment) for fragment in EmailsList if fragment)\n",
    "messages = np.array([message for message in msg_temp if len(message.split()) > 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Найдем часто встречаемые биграммы в датасете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('department', 'state'), 1001), (('state', 'dept'), 991), (('dept', 'produced'), 991), (('produced', 'house'), 991), (('house', 'select'), 991), (('agreement', 'information'), 991), (('information', 'redaction'), 991), (('redaction', 'foia'), 991), (('case', 'date'), 980), (('select', 'benghazi'), 979)]\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice, zip_longest\n",
    "from collections import Counter\n",
    "\n",
    "words = re.findall(\"\\w+\", str(wordsBigr))\n",
    "print(Counter(zip(words, islice(words, 1, None))).most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдем коллокации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_fibril', 'prolonged'),\n",
       " ('abetlin', 'hum'),\n",
       " ('adms', 'stavridis'),\n",
       " ('africom', 'planner'),\n",
       " ('alon', 'olson'),\n",
       " ('ambs', 'serbia'),\n",
       " ('annoyance', 'qataar'),\n",
       " ('anthony', 'cordesinan'),\n",
       " ('apache', 'longbow'),\n",
       " ('ashamed', 'shalgam'),\n",
       " ('assembly', 'terry'),\n",
       " ('attract', 'investor'),\n",
       " ('baitul', 'mokarram'),\n",
       " ('bark', 'ruggles'),\n",
       " ('bashing', 'hammer'),\n",
       " ('bassador', 'chri'),\n",
       " ('belated', 'honoring'),\n",
       " ('bianna', 'golodryga'),\n",
       " ('blow', 'qaddall'),\n",
       " ('bondy', 'tsou')]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(wordsBigr)\n",
    "finder.nbest(bigram_measures.pmi, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(993, 804)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=500, min_df=10)\n",
    "matrix = vectorizer.fit_transform(messages).toarray()\n",
    "print(matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 7, 4, 7, 4, 7, 4, 1, 4, 0, 0, 4, 7, 4, 7, 0, 7, 4, 0, 4, 1, 4, 1, 4, 1, 4, 7, 4, 7, 0, 7, 4, 0, 4, 5, 4, 3, 4, 3, 4, 1, 4, 2, 4, 2, 4, 2, 4, 7, 4, 3, 4, 1, 4, 2, 4, 2, 4, 2, 4, 7, 4, 1, 4, 0, 4, 0, 4, 0, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 2, 4, 2, 4, 2, 4, 7, 4, 1, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 1, 4, 1, 4, 2, 4, 2, 4, 2, 4, 2, 4, 1, 4, 1, 4, 1, 4, 3, 4, 3, 4, 0, 4, 8, 4, 1, 4, 1, 1, 4, 7, 4, 1, 4, 7, 4, 7, 4, 2, 4, 7, 4, 1, 4, 4, 4, 1, 4, 4, 4, 4, 4, 5, 4, 1, 4, 1, 7, 4, 7, 4, 1, 4, 1, 7, 4, 7, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 1, 4, 3, 4, 3, 4, 1, 4, 4, 4, 3, 4, 3, 4, 1, 4, 3, 4, 3, 4, 1, 4, 1, 4, 3, 4, 3, 4, 3, 4, 8, 4, 3, 4, 3, 4, 3, 4, 2, 4, 2, 4, 7, 4, 2, 4, 2, 4, 7, 4, 1, 4, 3, 4, 3, 4, 3, 4, 1, 4, 3, 4, 3, 4, 3, 4, 3, 4, 5, 4, 1, 4, 8, 4, 3, 4, 3, 4, 0, 4, 1, 4, 3, 4, 3, 4, 9, 4, 0, 4, 1, 4, 1, 4, 1, 4, 1, 4, 3, 4, 3, 4, 0, 4, 1, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 1, 4, 0, 4, 5, 4, 5, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 1, 4, 6, 4, 6, 4, 6, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 1, 4, 3, 4, 3, 4, 3, 4, 7, 4, 2, 4, 2, 4, 8, 4, 3, 4, 3, 4, 3, 4, 1, 4, 0, 4, 1, 4, 7, 4, 2, 4, 2, 4, 7, 4, 2, 4, 2, 4, 2, 4, 1, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 1, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 1, 4, 8, 4, 1, 4, 1, 4, 1, 4, 1, 4, 6, 4, 8, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 1, 4, 1, 4, 1, 4, 3, 4, 1, 4, 3, 4, 1, 4, 4, 4, 1, 4, 1, 4, 1, 4, 0, 4, 8, 4, 1, 4, 1, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 8, 4, 1, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 4, 1, 4, 1, 4, 1, 4, 8, 4, 8, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 0, 4, 0, 5, 8, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 2, 4, 2, 4, 2, 4, 2, 4, 3, 4, 1, 4, 0, 4, 2, 4, 2, 4, 2, 4, 2, 4, 1, 4, 1, 4, 0, 4, 2, 4, 2, 4, 4, 4, 3, 4, 3, 4, 9, 4, 9, 4, 9, 4, 9, 4, 9, 4, 2, 4, 2, 4, 1, 4, 1, 4, 8, 4, 2, 4, 2, 4, 2, 4, 2, 4, 4, 4, 3, 4, 3, 4, 4, 4, 3, 4, 3, 4, 2, 4, 2, 4, 9, 4, 9, 4, 9, 4, 9, 4, 9, 4, 9, 4, 9, 4, 9, 4, 9, 4, 9, 4, 4, 4, 3, 4, 3, 4, 1, 4, 2, 4, 2, 4, 9, 4, 9, 4, 9, 4, 9, 4, 9, 4, 9, 4, 9, 4, 1, 4, 2, 4, 2, 4, 2, 4, 2, 4, 9, 4, 9, 4, 9, 4, 9, 4, 9, 4, 9, 4, 9, 4, 8, 4, 0, 4, 0, 4, 0, 4, 1, 4, 1, 4, 6, 4, 6, 4, 1, 4, 8, 4, 0, 4, 0, 4, 0, 4, 2, 4, 2, 4, 2, 4, 2, 4, 4, 4, 4, 4, 6, 4, 3, 4, 3, 4, 1, 4, 1, 4, 6, 4, 2, 4, 2, 4, 2, 4, 6, 4, 0, 4, 1, 4, 5, 4, 0, 4, 6, 4, 0, 4, 8, 4, 0, 4, 0, 4, 0, 4, 6, 4, 6, 4, 5, 4, 1, 4, 9, 4, 9, 4, 9, 4, 9, 4, 9, 4, 6]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster.hierarchical import AgglomerativeClustering\n",
    "\n",
    "model = AgglomerativeClustering(n_clusters=10,affinity='euclidean', linkage='complete')\n",
    "preds = model.fit_predict(matrix)\n",
    "print(list(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99999999999999944"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=800)\n",
    "features = svd.fit_transform(matrix)\n",
    "svd.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 0 2 0 2 0 2 0 4 0 4 4 0 0 0 2 0 2 0 0 0 4 0 0 0]\n",
      "=========================\n",
      "[9 1 5 1 5 1 5 1 9 1 9 1 1 1 1 5 1 5 1 1 1 9 1 0 1]\n",
      "=========================\n",
      "[ 9  1 13  1 13  1 13  1  9  1  9  1  1 13  1 13  1 13  1  1  1  9  1  0  1]\n",
      "=========================\n",
      "[16  0  7  0  7  0  7  0 16  0 16  0  0  7  0  7  0  7  0  0  0 16  0  0  0]\n",
      "=========================\n",
      "[18  0 24  0 24  0 16  0 13  0 13  0  0 24  0 24  0 16  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "model = KMeans(n_clusters=5, random_state=1)\n",
    "preds = model.fit_predict(matrix)\n",
    "print(preds[:25])\n",
    "print(\"=========================\")\n",
    "model = KMeans(n_clusters=10, random_state=1)\n",
    "preds = model.fit_predict(matrix)\n",
    "print(preds[:25])\n",
    "print(\"=========================\")\n",
    "model = KMeans(n_clusters=15, random_state=1)\n",
    "preds = model.fit_predict(matrix)\n",
    "print(preds[:25])\n",
    "print(\"=========================\")\n",
    "model = KMeans(n_clusters=20, random_state=1)\n",
    "preds = model.fit_predict(matrix)\n",
    "print(preds[:25])\n",
    "print(\"=========================\")\n",
    "model = KMeans(n_clusters=25, random_state=1)\n",
    "preds = model.fit_predict(matrix)\n",
    "print(preds[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У 10 кластеров наилучший результат распределения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 1 5 1 5 1 5 1 9 1 9 1 1 1 1 5 1 5 1 1 1 9 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "model = KMeans(n_clusters=10, random_state=1)\n",
    "preds = model.fit_predict(matrix)\n",
    "print(preds[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster size:  116\n",
      "state said benghazi security attack department stevens house libya romney\n",
      "\n",
      "cluster size:  602\n",
      "state department house benghazi case date agreement dept produced select\n",
      "\n",
      "cluster size:  32\n",
      "sent september monica hanley state message call king mailto hanleymr\n",
      "\n",
      "cluster size:  32\n",
      "state rice president benghazi iran information gregory think government case\n",
      "\n",
      "cluster size:  4\n",
      "libya qaddafi egypt part crisis state islamist force source rebel\n",
      "\n",
      "cluster size:  109\n",
      "state source government magariaf libya security information benghazi minister force\n",
      "\n",
      "cluster size:  3\n",
      "ambassador rice press morning nation sullivan meet transcript state sent\n",
      "\n",
      "cluster size:  4\n",
      "death issue state bloomberg say sent october said mill cheryl\n",
      "\n",
      "cluster size:  20\n",
      "clinton secretary question yeah state think going people said know\n",
      "\n",
      "cluster size:  71\n",
      "state sent sullivan september benghazi jacob department house date information\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def topNWords(cluster,n):\n",
    "    joinedCluster = \" \".join(cluster)\n",
    "    words = joinedCluster.split()\n",
    "    count = Counter(words)\n",
    "    top_10 = count.most_common(10)\n",
    "    top_words = re.findall(r'\\b[a-z]\\w+\\b', str(top_10))\n",
    "    text_top = \" \".join(word for word in top_words)\n",
    "    \n",
    "    return text_top\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"cluster size: \", messages[preds == i].shape[0])\n",
    "    print(topNWords(messages[preds == i], 10))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster size:  3\n",
      "state david department case date dept produced house select benghazi\n",
      "cluster size:  867\n",
      "state benghazi department house information agreement case date produced dept\n",
      "cluster size:  6\n",
      "country security effort team need diplomacy state risk crisis event\n",
      "cluster size:  32\n",
      "sent september monica hanley state message call king mailto hanleymr\n",
      "cluster size:  36\n",
      "magariaf cabinet minister state government president source abushagur according october\n",
      "cluster size:  8\n",
      "death leader laden state source libya concerned group aqim information\n",
      "cluster size:  5\n",
      "source official attack group magariaf state information benghazi security government\n",
      "cluster size:  8\n",
      "libya state source aqim rebel weapon report concerned intended equipment\n",
      "cluster size:  8\n",
      "source security magariaf attack state consulate government cover militia benghazi\n",
      "cluster size:  20\n",
      "clinton secretary question yeah state think going people said know\n"
     ]
    }
   ],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "clustering = GaussianMixture(n_components=10)\n",
    "clustering.fit(features)\n",
    "preds = clustering.predict(features)\n",
    "for i in range(10):\n",
    "    print(\"cluster size: \", messages[preds == i].shape[0])\n",
    "    print(topNWords(messages[preds == i], 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
